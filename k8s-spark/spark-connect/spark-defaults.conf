## tell Spark which cluster manager to talk to. In this case, the K8S API server
spark.master                              k8s://https://kubernetes.default.svc
spark.kubernetes.namespace                spark-operator
spark.kubernetes.authenticate.driver.serviceAccountName  spark-all-purpose-driver1
spark.kubernetes.container.image          docker.io/library/spark:4.0.0
## Only the driver will be cleaned up when the deployment is deleted, but the executors are created by the driver.
## Set it to true if you want the executors to be removed after termination; otherwise, we can also configure ownerReferences on executor pods
## Set it to false to maintain the terminated executors if you want to check the logs after termination.
## For manual clean up:
## kubectl get pods -n spark-operator -l spark-role=executor --show-labels | grep ContainerCreating
## kubectl get pods -n spark-operator -l spark-role=executor | grep -E "Error|Completed" | awk '{print "kubectl delete pod "$1" -n spark-operator"}' | bash
## kubectl delete pods -n spark-operator -l spark-role=executor -o jsonpath='{range .items[?(@.status.containerStatuses[*].state.waiting.reason=="ContainerCreating")]}{.metadata.name}{"\n"}{end}'
## Clean up config maps:
## kubectl get configmaps -n spark-operator -l spark-role=executor
## kubectl delete configmaps -n spark-operator -l spark-role=executor
spark.kubernetes.executor.deleteOnTermination: false

# Driver reachability
## - localhost (127.0.0.1) refers specifically to the local machine via a private internal loopback interface, which is
## only accessible by processes running on the host machine itself. Not reachable from other devices on the same network or the internet.
## - 0.0.0.0 is a "wildcard" meta-address used when binding a service to listen for connections on all available network interfaces (internal and external)
## It accepts connections on all IPv4 addresses assigned to the machine, including the loopback address (127.0.0.1),
## the local area network (LAN) IP address (e.g., 192.168.x.x), and any public IP addresses.
## This allows the driver to accept connections from any network interface (e.g., eth0 inside the container), not just localhost
spark.driver.bindAddress             0.0.0.0
spark.driver.host                    spark-connect1-svc.spark-operator.svc.cluster.local
spark.driver.port                    7078
spark.blockManager.port              7079

# To support Spark History Server
spark.eventLog.enabled:         true
spark.eventLog.dir:             file:///tmp/spark-events

spark.executor.cores                    1
spark.executor.memory                   1g

# Dynamic Allocation (Databricks-like behavior)
spark.dynamicAllocation.enabled           true
## In K8S setup, there is no external shuffle svc
spark.dynamicAllocation.shuffleTracking.enabled true
## In K8S setup: keep this false
spark.shuffle.service.enabled             false
## The driver pod name is like `spark-connect-b4f58bdd4-r2pg9`
## The executor pod name is like `spark-connect-server-b1af2d9a7424b308-exec-1`
spark.dynamicAllocation.initialExecutors  1
spark.dynamicAllocation.minExecutors      1
spark.dynamicAllocation.maxExecutors      50
## Keep executors warm longer
spark.dynamicAllocation.executorIdleTimeout           900s
## Set to "infinity" to never remove executors with cached data
spark.dynamicAllocation.cachedExecutorIdleTimeout     86400s
## There have been pending tasks backlogged for more than this duration, new executors will be requested.
## https://spark.apache.org/docs/latest/job-scheduling.html#request-policy
## spark.dynamicAllocation.schedulerBacklogTimeout              10s
## spark.dynamicAllocation.sustainedSchedulerBacklogTimeout     20s

# Tame pod churn on K8S
spark.kubernetes.allocation.batch.size         5
spark.kubernetes.allocation.executor.timeout   120s

# Make Spark Connect the front door
spark.connect.grpc.binding.port                15002

# Configs for multiple users to share this driver
spark.scheduler.mode                           FAIR
spark.sql.adaptive.enabled                     true
spark.sql.adaptive.coalescePartitions.enabled  true