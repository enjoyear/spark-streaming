# K8S Spark configurations can be found here https://spark.apache.org/docs/latest/running-on-kubernetes.html#spark-properties
## tell Spark which cluster manager to talk to. In this case, the K8S API server
spark.master                                k8s://https://kubernetes.default.svc
spark.kubernetes.namespace                  spark-operator
spark.kubernetes.authenticate.driver.serviceAccountName  spark-all-purpose-driver1

# image for both driver and executors
# or configure `spark.kubernetes.driver.container.image` and `spark.kubernetes.executor.container.image` separately
spark.kubernetes.container.image            docker.io/library/spark:4.0.0
# Option 1: include custom jars in $SPARK_HOME/jars directory or subdirectory
# spark.kubernetes.container.image            spark-wordcount:v2

# Option 2: use `spark.jars` to distribute jars
# 1. Driver startup: Spark scans the spark.jars paths on driver
# 2. Executor provisioning: Driver creates executor pods via K8s API
# 3. JAR upload: Driver starts an HTTP file server
# 4. Executor startup: Executors download JARs from driver's HTTP server to their local cache
# 5. Classpath setup: Executors add downloaded JARs to their classpath
# You'll see lines like below in the log
# INFO Utils: Fetching spark://all-purpose1-svc.spark-operator.svc.cluster.local:7078/jars/word-count.jar to /var/data/spark-b8411c50-523b-477e-95d5-d0319f285ac6/spark-557cf305-8835-4e97-b235-b74a38111e26/fetchFileTemp15737109595795909693.tmp
# INFO Utils: Copying /var/data/spark-b8411c50-523b-477e-95d5-d0319f285ac6/spark-557cf305-8835-4e97-b235-b74a38111e26/-1314790651764484175511_cache to /opt/spark/work-dir/./word-count.jar
# INFO Executor: Adding file:/opt/spark/work-dir/./word-count.jar to class loader default
spark.jars=file:///opt/spark/jars/custom/word-count.jar

# # Define init container for executors
# spark.kubernetes.executor.initContainers.copyJars.image=busybox:1.36
# spark.kubernetes.executor.initContainers.copyJars.command[0]=sh
# spark.kubernetes.executor.initContainers.copyJars.command[1]=-c
# spark.kubernetes.executor.initContainers.copyJars.command[2]=cp /custom-jars/* /opt/spark/jars/custom/
#
# # Mount volumes for init container
# spark.kubernetes.executor.volumes.persistentVolumeClaim.custom-jars.mount.path=/custom-jars
# spark.kubernetes.executor.volumes.persistentVolumeClaim.custom-jars.options.claimName=custom-jars-pvc
#
# # Mount emptyDir for sharing with executor container
# spark.kubernetes.executor.volumes.emptyDir.shared-jars.mount.path=/opt/spark/jars/custom
# spark.kubernetes.executor.volumes.emptyDir.shared-jars.medium=Memory

## Only the driver will be cleaned up when the deployment is deleted, but the executors are created by the driver.
## Set it to true if you want the executors to be removed after termination; otherwise, we can also configure ownerReferences on executor pods
## Executors will start and fail until driver can be connected.
## Set it to false to maintain the terminated executors if you want to check the logs after termination.
## For manual clean up:
## kubectl get pods -n spark-operator -l spark-role=executor --show-labels | grep ContainerCreating
## kubectl get pods -n spark-operator -l spark-role=executor | grep -E "Error|Completed" | awk '{print "kubectl delete pod "$1" -n spark-operator"}' | bash
## kubectl delete pods -n spark-operator -l spark-role=executor -o jsonpath='{range .items[?(@.status.containerStatuses[*].state.waiting.reason=="ContainerCreating")]}{.metadata.name}{"\n"}{end}'
## Clean up config maps:
## kubectl get configmaps -n spark-operator -l spark-role=executor
## kubectl delete configmaps -n spark-operator -l spark-role=executor
spark.kubernetes.executor.deleteOnTermination       true
spark.kubernetes.executor.podNamePrefix             all-purpose1

spark.kubernetes.allocation.batch.size                  5
## Don't provision executors too fast, especially on application start when driver is not entirely ready
spark.kubernetes.allocation.batch.delay                 3s
## Time to wait for driver pod to get ready before creating executor pods.
## This wait only happens on application start. If timeout happens, executor pods will still be created.
## Description is the closest, but still useless
spark.kubernetes.allocation.driver.readinessTimeout     60s
spark.kubernetes.allocation.executor.timeout            120s

# Driver reachability
## - localhost (127.0.0.1) refers specifically to the local machine via a private internal loopback interface, which is
## only accessible by processes running on the host machine itself. Not reachable from other devices on the same network or the internet.
## - 0.0.0.0 is a "wildcard" meta-address used when binding a service to listen for connections on all available network interfaces (internal and external)
## It accepts connections on all IPv4 addresses assigned to the machine, including the loopback address (127.0.0.1),
## the local area network (LAN) IP address (e.g., 192.168.x.x), and any public IP addresses.
## This allows the driver to accept connections from any network interface (e.g., eth0 inside the container), not just localhost
spark.driver.bindAddress             0.0.0.0
spark.driver.host                    all-purpose1-svc.spark-operator.svc.cluster.local
spark.driver.port                    7078
spark.blockManager.port              7079

# To support Spark History Server
spark.eventLog.enabled:         true
spark.eventLog.dir:             file:///tmp/spark-events

spark.executor.cores                    1
spark.executor.memory                   1g

# Dynamic Allocation (Databricks-like behavior)
spark.dynamicAllocation.enabled           true
## In K8S setup, there is no external shuffle svc
spark.dynamicAllocation.shuffleTracking.enabled true
## In K8S setup: keep this false
spark.shuffle.service.enabled             false
## The driver pod name is like `all-purpose1-b4f58bdd4-r2pg9`
## The executor pod name is like `all-purpose1-exec-1`
spark.dynamicAllocation.initialExecutors  1
spark.dynamicAllocation.minExecutors      1
spark.dynamicAllocation.maxExecutors      50
## Keep executors warm longer
spark.dynamicAllocation.executorIdleTimeout           900s
## Set to "infinity" to never remove executors with cached data
spark.dynamicAllocation.cachedExecutorIdleTimeout     86400s
## There have been pending tasks backlogged for more than this duration, new executors will be requested.
## https://spark.apache.org/docs/latest/job-scheduling.html#request-policy
## spark.dynamicAllocation.schedulerBacklogTimeout              10s
## spark.dynamicAllocation.sustainedSchedulerBacklogTimeout     20s

# Make Spark Connect the front door
spark.connect.grpc.binding.port                15002
spark.connect.session.timeout                  30m

# Configs for multiple users to share this driver
spark.scheduler.mode                           FAIR
spark.sql.adaptive.enabled                     true
spark.sql.adaptive.coalescePartitions.enabled  true