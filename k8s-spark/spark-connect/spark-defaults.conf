## tell Spark which cluster manager to talk to. In this case, the K8S API server
spark.master                              k8s://https://kubernetes.default.svc
spark.kubernetes.namespace                spark-operator
spark.kubernetes.authenticate.driver.serviceAccountName  spark-all-purpose-driver1
spark.kubernetes.container.image          docker.io/library/spark:4.0.0
## Only the driver will be cleaned up when the deployment is deleted, but the executors are created by the driver.
## Set it to true if you want the executors to be removed after termination; otherwise, we can also configure ownerReferences on executor pods
## Set it to false to maintain the terminated executors if you want to check the logs after termination.
## For manual clean up:
## kubectl get pods -n spark-operator -l spark-role=executor --show-labels | grep ContainerCreating
## kubectl get pods -n spark-operator -l spark-role=executor | grep ContainerCreating | awk '{print "kubectl delete pod "$1" -n spark-operator"}' | bash
## kubectl delete pods -n spark-operator -l spark-role=executor -o jsonpath='{range .items[?(@.status.containerStatuses[*].state.waiting.reason=="ContainerCreating")]}{.metadata.name}{"\n"}{end}'
## Clean up config maps:
## kubectl get configmaps -n spark-operator -l spark-role=executor
## kubectl delete configmaps -n spark-operator -l spark-role=executor
spark.kubernetes.executor.deleteOnTermination: false

# To support Spark History Server
spark.eventLog.enabled:         true
spark.eventLog.dir:             file:///tmp/spark-events

## The maximum number of executor failures allowed before the Spark Connect application is marked as failed
spark.max.executor.failures             2
spark.executor.cores                    1
spark.executor.memory                   1g

# Dynamic Allocation (Databricks-like behavior)
spark.dynamicAllocation.enabled           true
## In K8S setup, there is no external shuffle svc
spark.dynamicAllocation.shuffleTracking.enabled true
## In K8S setup: keep this false
spark.shuffle.service.enabled             false
## The driver pod name is like `spark-connect-b4f58bdd4-r2pg9`
## The executor pod name is like `spark-connect-server-b1af2d9a7424b308-exec-1`
spark.dynamicAllocation.initialExecutors  1
spark.dynamicAllocation.minExecutors      1
spark.dynamicAllocation.maxExecutors      50
## Keep executors warm longer
spark.dynamicAllocation.executorIdleTimeout           900s
## Set to "infinity" to never remove executors with cached data
spark.dynamicAllocation.cachedExecutorIdleTimeout     86400s
## There have been pending tasks backlogged for more than this duration, new executors will be requested.
## https://spark.apache.org/docs/latest/job-scheduling.html#request-policy
spark.dynamicAllocation.schedulerBacklogTimeout       1s

# Tame pod churn on K8S
spark.kubernetes.allocation.batch.size         5
spark.kubernetes.allocation.executor.timeout   120s

# Make Spark Connect the front door
spark.connect.grpc.binding.port                15002

# Configs for multiple users to share this driver
spark.scheduler.mode                           FAIR
spark.sql.adaptive.enabled                     true
spark.sql.adaptive.coalescePartitions.enabled  true