{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedddc79-44d4-4548-9726-1658b7cd752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio\n",
      "  Downloading grpcio-1.76.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.whl.metadata (3.7 kB)\n",
      "Collecting typing-extensions~=4.12 (from grpcio)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Downloading grpcio-1.76.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (6.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.4/6.4 MB\u001B[0m \u001B[31m32.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.6/44.6 kB\u001B[0m \u001B[31m11.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: typing-extensions, grpcio\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed grpcio-1.76.0 typing-extensions-4.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install grpcio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a4c31-2a5f-4d9b-95b4-7d608e7486ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install grpc_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932b2cb7-0bbe-4cd4-8f42-5dc672797b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark[connect] in /usr/local/spark/python (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark[connect]) (0.10.9.7)\n",
      "Collecting googleapis-common-protos>=1.56.4 (from pyspark[connect])\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting grpcio-status>=1.56.0 (from pyspark[connect])\n",
      "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting grpcio>=1.56.0 (from pyspark[connect])\n",
      "  Downloading grpcio-1.76.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.11/site-packages (from pyspark[connect]) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/lib/python3.11/site-packages (from pyspark[connect]) (2.0.3)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from pyspark[connect]) (13.0.0)\n",
      "Requirement already satisfied: protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /opt/conda/lib/python3.11/site-packages (from googleapis-common-protos>=1.56.4->pyspark[connect]) (4.24.3)\n",
      "Collecting typing-extensions~=4.12 (from grpcio>=1.56.0->pyspark[connect])\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from googleapis-common-protos>=1.56.4->pyspark[connect])\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_aarch64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.5->pyspark[connect]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.5->pyspark[connect]) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.5->pyspark[connect]) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[connect]) (1.16.0)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m297.5/297.5 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading grpcio-1.76.0-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.whl (6.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.4/6.4 MB\u001B[0m \u001B[31m17.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading grpcio_status-1.76.0-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_aarch64.whl (324 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m324.4/324.4 kB\u001B[0m \u001B[31m13.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.6/44.6 kB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: typing-extensions, protobuf, grpcio, googleapis-common-protos, grpcio-status\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.3\n",
      "    Uninstalling protobuf-4.24.3:\n",
      "      Successfully uninstalled protobuf-4.24.3\n",
      "Successfully installed googleapis-common-protos-1.72.0 grpcio-1.76.0 grpcio-status-1.76.0 protobuf-6.33.0 typing-extensions-4.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark[connect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869c3060-764f-40f5-9cb6-a00d85b18ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark: 3.5.0 python: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 11:57:02) [GCC 12.3.0]\n"
     ]
    }
   ],
   "source": [
    "import pyspark, sys\n",
    "print(\"pyspark:\", pyspark.__version__, \"python:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd497d42-9d1e-4d79-83bd-235194e92ecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'grpc_status'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Create a Spark session using Connect\u001B[39;00m\n\u001B[1;32m      4\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43mSparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mremote\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msc://spark-connect1-svc.spark-operator.svc.cluster.local:15002\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mJupyterNotebook\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m----> 7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Test the connection\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpark version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspark\u001B[38;5;241m.\u001B[39mversion\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/session.py:464\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    459\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSPARK_CONNECT_MODE_ENABLED\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSPARK_REMOTE\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron\n\u001B[1;32m    461\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.remote\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m opts\n\u001B[1;32m    462\u001B[0m ):\n\u001B[1;32m    463\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m--> 464\u001B[0m         \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msession\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession \u001B[38;5;28;01mas\u001B[39;00m RemoteSparkSession\n\u001B[1;32m    466\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    467\u001B[0m             SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    468\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m SparkSession\u001B[38;5;241m.\u001B[39m_instantiatedSession \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    469\u001B[0m         ):\n\u001B[1;32m    470\u001B[0m             url \u001B[38;5;241m=\u001B[39m opts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.remote\u001B[39m\u001B[38;5;124m\"\u001B[39m, os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSPARK_REMOTE\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/connect/session.py:54\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf, __version__\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkConnectClient, ChannelBuilder\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RuntimeConf\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataFrame\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/connect/client/__init__.py:22\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_dependencies\n\u001B[1;32m     20\u001B[0m check_dependencies(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclient\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa: F401,F403\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/spark/python/pyspark/sql/connect/client/core.py:60\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpa\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmessage\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgrpc_status\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rpc_status\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgrpc\u001B[39;00m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m text_format\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'grpc_status'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session using Connect\n",
    "spark = SparkSession.builder \\\n",
    "    .remote(\"sc://all-purpose1-svc.spark-operator.svc.cluster.local:15002\") \\\n",
    "    .appName(\"JupyterNotebook\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Test the connection\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark Connect session active: {spark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c903e99-f204-4290-b965-582c10fd1f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a619625-a729-4b14-a81e-3c380910d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Configuration ===\n",
      "App Name: JupyterNotebook\n",
      "SQL Shuffle Partitions: 20\n",
      "Updated: 201\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Configuration ===\")\n",
    "print(f\"App Name: {spark.conf.get('spark.app.name')}\")\n",
    "print(f\"SQL Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"201\")\n",
    "print(f\"Updated: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdda8e6-0f77-46ac-a0de-61c0380bd224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
